---
title: "DM Project"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Student's Adaptability Level in Online Education

------------------------------------------------------------------------

## 1. The Problem

The recent rise in usage of online education has brought up the problem of students' adaptability level. In this project, we aim to analyze and understand what attributes factor in the rate of adaptivity level in the hope of helping institutes along with students to develop strategies to enhance adaptability.

## 2. Data Mining Task

This project addresses the problem of students' adaptability in online education using two data mining tasks: classification and clustering. The classification task will predict and classify students using our class label "Adaptivity Level" into adaptability levels ("High", "Moderate", "Low"). The clustering task will present clusters of students based on shared characteristics, which will be used to predict new students' adaptability levels.

## 3. Data

### 3.1 Source

The dataset was obtained from Kaggle.\
<https://www.kaggle.com/datasets/mdmahmudulhasansuzan/students-adaptability-level-in-online-education>

### 3.2 General information

Our dataset has 14 attributes and 1,205 objects.\
The class label is "Adaptivity Level".

### 3.3 Attributes characteristics

|   Attribute Name    | Attribute Type |                    Description                    |            Possible Values            |
|:-------------:|:-------------:|:----------------------:|:----------------:|
|       Gender        |     Binary     |               Student's gender type               |               Girl, Boy               |
|         Age         |    Ordinal     |                Student's age range                | 1-5, 6-10, 11-15, 16-20, 21-25, 26-30 |
|  Educational Level  |    Nominal     |       Student's education institution level       |      School, College, University      |
|  Institution Type   |     Binary     |       Student's education institution type        |      Government, Non-government       |
|     IT Student      |     Binary     |     Whether the student is studying IT or not     |                Yes, No                |
|      Location       |     Binary     | Whether the student is studying in their hometown |                Yes, No                |
|    Load-shedding    |     Binary     |              Level of load-shedding               |               High, Low               |
| Financial Condition |    Ordinal     |      Student's family's financial condition       |            Rich, Mid, Poor            |
|    Internet Type    |     Binary     |         Student's most used internet type         |           Wifi, Mobile Data           |
|    Network Type     |    Ordinal     |             Network connectivity type             |              2G, 3G, 4G               |
|   Class Duration    |    Ordinal     |      Student's daily class duration in hours      |              0, 1-3, 3-6              |
|      Self LMS       |     Binary     | Whether the student's institution has its own LMS |                Yes, No                |
|       Device        |    Nominal     |        Student's most used device in class        |         Computer, Tab, Mobile         |
|  Adaptivity Level   |    Ordinal     | Student's adaptibility level to online education  |          High, Moderate, Low          |

------------------------------------------------------------------------

## 4. Data Summarization and Preprocessing:

```{r setup, include=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com"))
```

### 4.1 Installing the packages:

```{r}
install.packages("dplyr")
install.packages("farver")
install.packages("ggplot2")
install.packages("colorspace")
install.packages("scatterplot3d")
install.packages("gridExtra")
install.packages("superml")
install.packages('party')
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
install.packages("NbClust")
install.packages("C50")
```

### 4.2 Reading libraries and the dataset:

```{r}
library(rpart)
library(rpart.plot)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(scatterplot3d)
library(ggplot2)
library(dplyr)
library(readr)
library(gridExtra)
library('superml')
library(caret)
library(party)
library(fpc)
library(FSelector)
library(cluster)
library(factoextra)
library('DPBBM')
library("NbClust")
library("C50")
```

```{r}
dataset <- read_csv("Dataset/students_adaptability_level_online_education.csv")
```

### 4.3 Sample and summary of raw dataset:

```{r}
View(dataset)

shape<-dim(dataset)
row<-shape[1]
col<-shape[2]
cat("Number of instances:", row, "\n")
cat("Number of attributes:", col, "\n")

summary(dataset)
names(dataset)
str(dataset)
```

### 4.4 Checking for missing values:

```{r}
sum(is.na(dataset))
```

Since there are no missing values, and no numerical data to be checked for outliers, and all attributes are of type char, we can't calculate variance or do discretization.

------------------------------------------------------------------------

### 4.5 Plotting Methods:

#### 4.5.1 Bar plot of the gender, age, and education level attributes:

```{r}
# Bar plot of the gender attribute
ggplot(dataset, mapping = aes(x = Gender)) +
  geom_bar(aes(fill = Gender), position = "dodge")

# Bar plot of the age attribute
ggplot(dataset, mapping = aes(x = Age)) +
  geom_bar(aes(fill = Age), position = "dodge")

# Bar plot of the education level attribute
ggplot(dataset, mapping = aes(x = `Education Level`)) +
  geom_bar(aes(fill = `Education Level`), position = "dodge")
```

Graphs descriptions:

Gender plot: this graph shows us that the number of students who are boys are slightly more than the girl students.

Age plot: this graph shows us the number of students whose data been collected based on their age range, we noticed they are mostly between the ages of 21-25 and 11-15.

Education level plot: this graph shows us that the dataset is mostly collected from school and university students, and only a small number was collected from college students.

------------------------------------------------------------------------

#### 4.5.2 Mosaic plot of the educational level attribute with the financial condition:

```{r}
#Mosaic plot of the educational level attribute with the financial condition
counts <- table(dataset$'Education Level', dataset$'Financial Condition')
mosaicplot(counts, xlab='Education Level', ylab='Financial Condition',
           main='Education Level by Financial Condition', col='forestgreen')
```

Graph description: this graph shows us a visual comparison between students' financial condition against there education level, we noticed that in each education level category, students mostly come from a middle-class family regarding their financial condition.

------------------------------------------------------------------------

#### 4.5.3 Pie chart for the self-LMS, IT student, and the adaptivity level attributes:

```{r}
# Pie chart for the self-LMS attribute
tab <- dataset$'Self Lms'%>% table()
precentages <- tab %>% prop.table() %>% round(2) * 100 
txt <- paste0(names(tab), '\n', precentages, '%') 
pie(tab, labels=txt, main = "Distribution of self-LMS institutes")

# Pie chart for the IT student attribute
tab <- dataset$'IT Student'%>% table()
precentages <- tab %>% prop.table() %>% round(2) * 100 
txt <- paste0(names(tab), '\n', precentages, '%') 
pie(tab, labels=txt, main = "Distribution of IT students")

# Pie chart for the adaptivity level attribute
tab <- dataset$'Adaptivity Level' %>% table()
percentages <- tab %>% prop.table() %>% round(3) * 100
txt <- paste0(names(tab), '\n', percentages, '%')
pie(tab, labels = txt, main = "Distribution of Adaptivity Levels")

```

Graphs descriptions:

Self-LMS chart: the graph shows us the percentage of institutions that has their own LMS, we notice that most of them, by a 83% percentage, do not have their own.

IT-Student chart: the graph shows us the percentage of students who are studying IT, we notice that most of them, by a 75% percentage, are not.

Adaptivity level chart: the graphs shows us the percentages of students adaptivity level in online education, which is our class label, we notice that most of them, by a 51.9% percentage, have a moderate adaptivity level, 39.8% have a low adaptivity level, and only 8.3% have a high adaptivity level.

------------------------------------------------------------------------

Since this dataset only contains categorical data, discreization and normalization cannot be performed.

------------------------------------------------------------------------

### 4.6 Encoding categorical variables:

```{r}
# Encoding categorical attributes
categorical_cols <- dataset %>%
  select_if(is.character) %>%
  names()
categorical_cols
# Convert categorical columns to factors
dataset[categorical_cols] <- lapply(dataset[categorical_cols], as.factor)
```

------------------------------------------------------------------------

### 4.7 Chi Square:

```{r}
chisq.test(dataset$`Institution Type`, dataset$`Financial Condition`)
```

since the p value is less than 0.05, therefore, the institution type depends on the financial condition.

------------------------------------------------------------------------

### 4.8 Balancing the dataset:

Since we can clearly see that the dataset is imbalanced from the piechart provided above, we must take action and balance the dataset, we therefore chose to over-sample the dataset by increasing instances were the class label result is "High", we will x4 its rows to achieve this.

```{r}
colnames(dataset) <- make.names(colnames(dataset))

# Selecting the High rows so we can triple them
high_rows <- dataset[dataset$Adaptivity.Level == "High", ]

# Triplicate the 'High' rows
triplicated_high_rows <- high_rows[rep(seq_len(nrow(high_rows)), each = 4), ]

# Combine the original dataset with the triplicated 'High' rows
balancedDataset <- rbind(dataset, triplicated_high_rows)

tab <- balancedDataset$Adaptivity.Level %>% table()
percentages <- tab %>% prop.table() %>% round(3) * 100
txt <- paste0(names(tab), '\n', percentages, '%')
pie(tab, labels = txt, main = "Distribution of Adaptivity Levels")

shape<-dim(balancedDataset)
row<-shape[1]
cat("Number of instances:", row)
```

As we can see, the dataset looks so much better now and is nearly perfectly balanced, after adding 4 times the original amount of rows with "High" adaptivity level, with the total of 1605 instances instead of the previous 1205.

------------------------------------------------------------------------

## 5. Data Mining Techniques:

### 5.1 Classification:

In this section, we will be building decision trees, which are a recursive method that generate a tree with leaf nodes pointing to the final decision. Our model will predict the adaptivity level classification ("High", "Moderate", "Low"). The rest of the attributes are used to make the prediction. To be consistent we used the same formula for all methods except Gain ratio; since the decision tree will be complex, so we be content with Job Status, Job Rate , Department and Permissions which are the most 4 important attributes as mentioned in the feature selection.

We will be dividing the dataset three times by 3 different distributions into two set, one for training the model, and the other is to test it, the distributions were 90/10, 80/20, and 70/30, with the training test having the bigger percentage in all distributions.

The choice of dividing the dataset into three different distributions (90/10, 80/20, and 70/30) reflects a deliberate strategy to explore varying proportions of training and testing data. Each distribution represents a trade-off between the amount of data available for training, crucial for the model to learn patterns effectively, and the amount reserved for testing, essential for evaluating the model's performance on unseen data. The 90/10 distribution allocates a substantial portion for training, enabling the model to capture intricate patterns in the data. The 80/20 distribution strikes a balance, providing a diverse training set while maintaining a sizable testing set. The 70/30 distribution allows for a more comprehensive evaluation, with a substantial proportion dedicated to testing. This systematic exploration across distributions aims to discern how the model generalizes across varying amounts of training data and assesses its robustness under different scenarios. The chosen distributions provide a nuanced perspective on the model's performance, aiding in comprehensive insights and informed decision-making.

We will also be using 3 different evaluation metrics, Information Gain, Gain Ratio, and Gini Index.

Information Gain is a measure that quantifies the reduction in uncertainty about the target variable achieved by splitting the data based on a particular feature. The goal of a decision tree is to select the feature that provides the highest information gain, as it leads to more effective classification. Information gain is calculated by comparing the entropy (or impurity) of the dataset before and after the split. We will be using rpart to achieve this.

Gain Ratio is an extension of Information Gain that takes into account the intrinsic information of a feature, preventing bias towards features with many values. Gain Ratio is computed by dividing the Information Gain by the split information, which measures the potential information provided by a feature. We will be using C5.0 to achieve this.

Gini Index is another impurity measure used in decision trees. It quantifies the probability of incorrectly classifying a randomly chosen element in the dataset. The Gini Index is minimized when a node is pure, meaning all instances belong to a single class. Decision trees aim to select splits that minimize the Gini Index, leading to nodes that are as homogenous as possible. We will be using rpart to achieve this.

#### 5.1.1 90/10 distribution:

Splitting the data into training and testing tests

```{r}
set.seed(1234)

split90 <- createDataPartition(balancedDataset$Adaptivity.Level, p = 0.9, list = FALSE, times = 1)
trainData <- balancedDataset[split90,]
testData <- balancedDataset[split90,]
```

Building decision tress for each attribute selection measure:

```{r}
infoGainTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "information"))
gainRatioTree <- C5.0(Adaptivity.Level ~ ., data = trainData)
GiniIndexTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "gini"))
```

1.  Information Gain

```{r}
IG1.ctree <- ctree(infoGainTree, data = trainData)
table(predict(IG1.ctree), trainData$Adaptivity.Level)

print(IG1.ctree)
plot(IG1.ctree, type="simple")

testPred <- predict(IG1.ctree, newdata = testData)
result<-table(testPred, testData$Adaptivity.Level)

results <- confusionMatrix(testPred, testData$Adaptivity.Level)
print(results)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("Precision:", precision)
```

Model Analysis:

This tree shows that the most influential attribute for identifying the Adaptivity Level is the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age.

The 90/10 split with Information Gain showed good results across the evaluation metrics, with accuracy at 80.69%, precision which is the positive predictive value 75.23%, sensitivity which is the true positive rate at 81.36%, and specificity which is the true negative rate at 90.28%, those are very good numbers and indicate a good model.

2.  Gain Ratio

```{r}
# Make predictions on the training set
trainPred <- predict(gainRatioTree, newdata = trainData)

# Display the confusion matrix for training data
trainResults <- confusionMatrix(trainPred, trainData$Adaptivity.Level)
print(trainResults)

# Make predictions on the test set
testPred <- predict(gainRatioTree, newdata = testData)

# Display the confusion matrix for test data
testResults <- confusionMatrix(testPred, testData$Adaptivity.Level)

# Plot the decision tree
plot(gainRatioTree)
summary(gainRatioTree)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("Precision:", precision, "\n")
```

Model Analysis:

The 90/10 split with Gain Ratio showed very good results across the evaluation metrics, with accuracy at 91.83%, precision which is the positive predictive value 92.36%, sensitivity which is the true positive rate at 92.42%, and specificity which is the true negative rate at 95.96%, those are very good numbers and indicate a great model, the root starts at the Class Duration attribute, followed by the Age and Financial Condition.

3.  Gini Index

```{r}
GI1.ctree <- ctree(GiniIndexTree, data = trainData)
table(predict(GI1.ctree), trainData$Adaptivity.Level)

print(GI1.ctree)
plot(GI1.ctree, type="simple")

testPred <- predict(GI1.ctree, newdata = testData)
result<-table(testPred, testData$Adaptivity.Level)

results <- confusionMatrix(testPred, testData$Adaptivity.Level)
print(results)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("Precision:", precision, "\n")
```

Model Analysis:

This tree shows the exact same results as the Information Gain tree, it shows that the most influential attribute for identifying the Adaptivity Level is the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age.

The 90/10 split with Gini Index, like the one with Information Gain showed good results across the evaluation metrics, with accuracy at 80.69%, precision which is the positive predictive value 75.23%, sensitivity which is the true positive rate at 81.36%, and specificity which is the true negative rate at 90.28%, those are very good numbers and indicate a good model.

#### 5.1.2 80/20 distribution:

Splitting the data into training and testing tests

```{r}
set.seed(1234)
split80 <- createDataPartition(balancedDataset$Adaptivity.Level, p = 0.8, list = FALSE, times = 1)
trainData <- balancedDataset[split80,]
testData <- balancedDataset[split80,]
```

Building decision tress for each attribute selection measure:

```{r}
infoGainTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "information"))
gainRatioTree <- C5.0(Adaptivity.Level ~ ., data = trainData)
GiniIndexTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "gini"))
```

1.  Information Gain

    ```{r}
    IG2.ctree <- ctree(infoGainTree, data = trainData)
    table(predict(IG2.ctree), trainData$Adaptivity.Level)

    print(IG2.ctree)
    plot(IG2.ctree, type="simple")

    testPred <- predict(IG2.ctree, newdata = testData)
    result<-table(testPred, testData$Adaptivity.Level)

    results <- confusionMatrix(testPred, testData$Adaptivity.Level)
    print(results)

    confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

    # Calculate precision
    precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
    cat("Precision:", precision)
    ```

    Model Analysis:

    This tree shows that the most influential attribute for identifying the Adaptivity Level is the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age. Those results are similar to the 90/10 distribution with IG/GI results.

    The 80/20 split with Information Gain showed lower results than the other trees in all evaluation metrics but precision, with accuracy at 77.88%, precision which is the positive predictive value 78.39%, sensitivity which is the true positive rate at 78.16%, and specificity which is the true negative rate at 88.80%, those are good numbers and indicate that the model is not a bad model.

2.  Gain Ratio

    ```{r}
    # Make predictions on the training set
    trainPred <- predict(gainRatioTree, newdata = trainData)

    # Display the confusion matrix for training data
    trainResults <- confusionMatrix(trainPred, trainData$Adaptivity.Level)
    print(trainResults)

    # Make predictions on the test set
    testPred <- predict(gainRatioTree, newdata = testData)

    # Display the confusion matrix for test data
    testResults <- confusionMatrix(testPred, testData$Adaptivity.Level)

    # Plot the decision tree
    plot(gainRatioTree)
    summary(gainRatioTree)

    confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

    # Calculate precision
    precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
    cat("\nPrecision:", precision)
    ```

Model Analysis:

The 80/20 split with Gain Ratio showed the best results across all evaluation metrics except precision, but it was still very high, with accuracy at 92.06%, precision which is the positive predictive value 91.93%, sensitivity which is the true positive rate at 92.58%, and specificity which is the true negative rate at 96.06%, those are very great numbers and indicate a great model, it is the best model we have built, and the root starts at the Class Duration attribute, followed by the Financial Condition, and then the Age.

3.  Gini Index

```{r}
GI2.ctree <- ctree(GiniIndexTree, data = trainData)
table(predict(GI2.ctree), trainData$Adaptivity.Level)

print(GI2.ctree)
plot(GI2.ctree, type="simple")

testPred <- predict(GI2.ctree, newdata = testData)
result<-table(testPred, testData$Adaptivity.Level)

results <- confusionMatrix(testPred, testData$Adaptivity.Level)
print(results)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("Precision:", precision, "\n")
```

Model Analysis:

This tree shows the exact same results as the Information Gain tree, it shows that the most influential attribute for identifying the Adaptivity Level is the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age.

The 80/20 split with Gini Index, like the one with Information Gain showed lower results than the other trees in all evaluation metrics but precision, with accuracy at 77.88%, precision which is the positive predictive value 78.39%, sensitivity which is the true positive rate at 78.16%, and specificity which is the true negative rate at 88.80%, those are good numbers and indicate that the model is not a model.

#### 5.1.3 70/30 distribution:

Splitting the data into training and testing tests

```{r}
set.seed(1234)
split70 <- createDataPartition(balancedDataset$Adaptivity.Level, p = 0.7, list = FALSE, times = 1)
trainData <- balancedDataset[split70,]
testData <- balancedDataset[split70,]
```

Building decision tress for each attribute selection measure:

```{r}
infoGainTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "information"))
gainRatioTree <- C5.0(Adaptivity.Level ~ ., data = trainData)
GiniIndexTree <- rpart(formula = Adaptivity.Level ~ ., data = trainData, method = "class", parms = list(split = "gini"))
```

1.  Information Gain

    ```{r}
    IG3.ctree <- ctree(infoGainTree, data = trainData)
    table(predict(IG3.ctree), trainData$Adaptivity.Level)

    print(IG3.ctree)
    plot(IG3.ctree, type="simple")

    testPred <- predict(IG3.ctree, newdata = testData)
    result<-table(testPred, testData$Adaptivity.Level)

    results <- confusionMatrix(testPred, testData$Adaptivity.Level)
    print(results)

    confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

    # Calculate precision
    precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
    cat("\nPrecision:", precision)
    ```

    1)  Model Analysis:

        This tree shows that the most influential attribute for identifying the Adaptivity Level is once again the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age. Those results are similar to the 90/10 and 80/20 distributions with IG/GI results.

        The 70/30 split with Information Gain showed average results compared to other trees in all evaluation metrics but precision, with accuracy at 78.83%, precision which is the positive predictive value 77.38%, sensitivity which is the true positive rate at 80%, and specificity which is the true negative rate at 89.51%, those are good numbers and indicate that the model is good.

2.  Gain Ratio

```{r}
# Make predictions on the training set
trainPred <- predict(gainRatioTree, newdata = trainData)

# Display the confusion matrix for training data
trainResults <- confusionMatrix(trainPred, trainData$Adaptivity.Level)
print(trainResults)

# Make predictions on the test set
testPred <- predict(gainRatioTree, newdata = testData)

# Display the confusion matrix for test data
testResults <- confusionMatrix(testPred, testData$Adaptivity.Level)

# Plot and Print the decision tree
plot(gainRatioTree)
summary(gainRatioTree)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("\nPrecision:", precision)
```

Model Analysis:

The 70/30 split with Gain Ratio showed good results across the evaluation metrics, with accuracy at 90.84%, precision which is the positive predictive value 92.56%, sensitivity which is the true positive rate at 91.47%, and specificity which is the true negative rate at 95.47%, those are very good numbers and indicate a very good model. The root starts at the Class Duration attribute, followed by the Age and Financial Condition.

3.  Gini Index

```{r}
GI3.ctree <- ctree(GiniIndexTree, data = trainData)
table(predict(GI3.ctree), trainData$Adaptivity.Level)

print(GI3.ctree)
plot(GI3.ctree, type="simple")

testPred <- predict(GI3.ctree, newdata = testData)
result<-table(testPred, testData$Adaptivity.Level)

results <- confusionMatrix(testPred, testData$Adaptivity.Level)
print(results)

confusion_matrix <- confusionMatrix(data = testPred, reference = testData$Adaptivity.Level)

# Calculate precision
precision <- confusion_matrix$table[2,2] / sum(confusion_matrix$table[,2])
cat("\nPrecision:", precision)
```

Model Analysis:

This tree shows the exact same results as the Information Gain tree, it shows that the most influential attribute for identifying the Adaptivity Level is the Financial Condition, it suggests that the first split is based on the financial condition being either "Mid" or "Poor." The subsequent splits are Class Duration being either "1-3" and "3-6", and then Age. Those results are similar to the 90/10 and 80/20 distributions with IG/GI results.

The 70/30 split with Gini Index, like the one with Information Gain showed average results compared to other trees in all evaluation metrics but precision, with accuracy at 78.83%, precision which is the positive predictive value 77.38%, sensitivity which is the true positive rate at 80%, and specificity which is the true negative rate at 89.51%, those are good numbers and indicate that the model is good.

------------------------------------------------------------------------

### 5.2 Clustering:

Clustering is a machine learning technique that involves grouping similar data points together based on inherent patterns and similarities. In this project, we leverage clustering to unveil hidden structures within our dataset, identifying natural groupings of students' attributes that can assist in understanding the intrinsic relationships between them. By employing clustering, we aim to reveal patterns that may not be immediately apparent in identifying students' adaptability level in online education.

We will start by removing our class label, then trying different methods to finding the optimal number of clusters, pick 3 numbers of K, which represent the number of clusters, and then finally validate our clusters based on their results on different metrics, namely the **Average Silhouette Width**, which measures how similar an object is to its own cluster compared to other clusters, the **Total Within-Cluster Sum of Square**, which quantifies the compactness of clusters, **BCubed Precision** and **BCubed Recall**, which are used to evaluate clustering performance, BCubed Precision measures the proportion of correctly clustered instances within a cluster, while BCubed Recall measures the proportion of correctly clustered instances for a given true class.

The results and findings will be presented afterwards in the Findings section.

#### 5.2.1 Removing the class label:

```{r}
# load the dataset 
df <- balancedDataset

#drop the class label (Adaptivity Level))
df<-df[1:13]
str(df)

distinct_data_points <- nrow(unique(df))
print(distinct_data_points)
```

#### 5.2.2 Finding the optimal number of clusters:

1- Using the Silhouette Method:

```{r}
# Convert dataframe columns to numeric
df_numeric <- as.data.frame(lapply(df, as.numeric))

df <- scale(df_numeric)
fviz_nbclust(df, kmeans, method = "silhouette") + labs(subtitle = "Silhouette method")
```

Graph description: K=2 seems to be the optimal number of clusters using the silhouette method.

2- Using the Elbow Method:

```{r}
wssplot<- function (df, nc=15, seed=1234)
{
  wss<-(nrow(df)-1)*sum(apply(df,2,var))
  for(i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(df, centers = i)$withinss)}
  plot(1:nc,wss, type="b", xlab="Number of clusters k", ylab="Within groups sum of squares")
}
df1 = na.omit(df)
wssplot(df1)
```

Graph description: Using the elbow method, K=2 also seems to be the optimal number of clusters.

As you can see, we employed two methods, the Silhouette method and the Elbow method, to assist in determining the optimal number of clusters for our dataset. Interestingly, both the Silhouette method and the Elbow method pointed towards 2 clusters as a potentially optimal choice. The Silhouette method, which measures how well-defined and separated the clusters are, suggested 2 as the most suitable number, while the Elbow method, examining the within-cluster sum of squares, repeated this recommendation. However, to ensure a comprehensive exploration, we also considered k values of 5 and 4 as they also seem good according to the elbow method. The inclusion of these additional cluster counts allows us to assess whether the clustering structure remains stable across different configurations.

#### 5.2.3 Performing K-Means Clustering with K=5, K=4, K=2:

```{r}

# Make this example reproducible
set.seed(1234)

# Determine the maximum value of k you want to try
max_k <- 5

# Create an empty list to store the results
kmeans_results <- list()

# Loop through different k values
for (k in 1:max_k) {
  set.seed(1234)  # Set seed inside the loop for reproducibility
  kmeans_results[[k]] <- kmeans(df_numeric, centers = k, nstart = 25)
}

```

1.  K = 5

```{r}
k5_results <- kmeans_results[[5]]
k5_results

# Visualize the clustering
fviz_cluster(k5_results, data = df_numeric, geom = "point")

# Calculate the silhouette width
silhouette_score <- silhouette(k5_results$cluster, dist(df_numeric))
# Extract the average silhouette width
average_silhouette <- mean(silhouette_score[, 3])

within_cluster_sum_squares <- k5_results$tot.withinss

# BCubed function
clusters <- k5_results$cluster
ground_truth <- c(balancedDataset$Adaptivity.Level)

BCubed_Precision <- function(clusters, ground_truth) {
  n <- length(clusters)
  precision <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
  }
  precision <- precision / n
  return(precision)
}

BCubed_Recall <- function(clusters, ground_truth) {
  n <- length(clusters)
  recall <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
  }
  recall <- recall / n
  return(recall)
}

precision <- BCubed_Precision(clusters, ground_truth)
recall <- BCubed_Recall(clusters, ground_truth)


# Display the results
cat("Average Silhouette width:", average_silhouette, "\n")
cat("Total within-cluster sum of squares:", within_cluster_sum_squares, "\n")
cat("BCubed precision:", precision, "\n")
cat("BCubed recall:", recall, "\n")
```

2.  K = 4

```{r}
k4_results <- kmeans_results[[4]]
k4_results

# Visualize the clustering
fviz_cluster(k4_results, data = df_numeric, geom = "point")

# Calculate the silhouette width
silhouette_score <- silhouette(k4_results$cluster, dist(df_numeric))
# Extract the average silhouette width
average_silhouette <- mean(silhouette_score[, 3])

within_cluster_sum_squares <- k4_results$tot.withinss

# BCubed function
clusters <- k4_results$cluster
ground_truth <- c(balancedDataset$Adaptivity.Level)

BCubed_Precision <- function(clusters, ground_truth) {
  n <- length(clusters)
  precision <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
  }
  precision <- precision / n
  return(precision)
}

BCubed_Recall <- function(clusters, ground_truth) {
  n <- length(clusters)
  recall <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
  }
  recall <- recall / n
  return(recall)
}

precision <- BCubed_Precision(clusters, ground_truth)
recall <- BCubed_Recall(clusters, ground_truth)


# Display the results
cat("Average Silhouette width:", average_silhouette, "\n")
cat("Total within-cluster sum of squares:", within_cluster_sum_squares, "\n")
cat("BCubed precision:", precision, "\n")
cat("BCubed recall:", recall, "\n")
```

3.  K = 2

    ```{r}
    k2_results <- kmeans_results[[2]]
    k2_results

    # Visualize the clustering
    fviz_cluster(k2_results, data = df_numeric, geom = "point")

    # Calculate the silhouette width
    silhouette_score <- silhouette(k2_results$cluster, dist(df_numeric))
    # Extract the average silhouette width
    average_silhouette <- mean(silhouette_score[, 3])

    within_cluster_sum_squares <- k2_results$tot.withinss

    # BCubed function
    clusters <- k2_results$cluster
    ground_truth <- c(balancedDataset$Adaptivity.Level)

    BCubed_Precision <- function(clusters, ground_truth) {
      n <- length(clusters)
      precision <- 0
      for (i in 1:n) {
        same_cluster <- clusters == clusters[i]
        same_ground_truth <- ground_truth == ground_truth[i]
        precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
      }
      precision <- precision / n
      return(precision)
    }

    BCubed_Recall <- function(clusters, ground_truth) {
      n <- length(clusters)
      recall <- 0
      for (i in 1:n) {
        same_cluster <- clusters == clusters[i]
        same_ground_truth <- ground_truth == ground_truth[i]
        recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
      }
      recall <- recall / n
      return(recall)
    }

    precision <- BCubed_Precision(clusters, ground_truth)
    recall <- BCubed_Recall(clusters, ground_truth)


    # Display the results
    cat("Average Silhouette width:", average_silhouette, "\n")
    cat("Total within-cluster sum of squares:", within_cluster_sum_squares, "\n")
    cat("BCubed precision:", precision, "\n")
    cat("BCubed recall:", recall, "\n")
    ```

------------------------------------------------------------------------

### 6. Evaluation and Comparison:

#### 6.1 Classification:

|     Distribution      |              90/10               |   90/10    |              80/20               |   80/20    |              70/30               |   70/30    |
|:--------:|:-----------:|:-----:|:-----------:|:-----:|:-------------:|:------:|
| **Evaluation Metric** | Information Gain and Gini Index  | Gain Ratio | Information Gain and Gini Index  | Gain Ratio | Information Gain and Gini Index  | Gain Ratio |
|     **Accuracy**      |              80.69%              |   91.83%   |              77.88%              |   92.06%   |              78.83%              |   90.84%   |
|     **Precision**     |              75.23%              |   92.36%   |              78.39%              |   91.93%   |              77.38%              |   92.56%   |
|    **Sensitivity**    |              81.36%              |   92.42%   |              78.16%              |   92.58%   |              80.00%              |   91.47%   |
|    **Specificity**    |              90.28%              |   95.96%   |              88.80%              |   96.06%   |              89.51%              |   95.47%   |

#### 6.2 Clustering:

|                                    |    K=2    |    K=4    |    K=5    |
|:----------------------------------:|:---------:|:---------:|:---------:|
|       Avg. Silhouette Width        | 0.3145231 | 0.253453  | 0.255273  |
| Total Within-Cluster Sum of Square | 5062.039  |  3933.21  | 3549.907  |
|          BCubed Precision          | 0.3607182 | 0.3787132 | 0.3906725 |
|           BCubed Recall            | 0.5356556 | 0.4328303 | 0.3172995 |

------------------------------------------------------------------------

### 7. Findings:

#### 7.1 Classification:

After some trials and research, we found that using Information Gain and Gini Index gave the exact same results in all distributions, this could be due to our dataset being relatively small and containing only categorical data.

We find from the collected and presented data that the accuracy ranges between 77.88% and 92.06, which overall shows that the models were good even if we go with the lowest value, precision was between 75.23% and 92.56%, sensitivity between 78.16% and 92.58%, and lastly, specificity between 88.80% and 96.06.

Using **Gain Ratio** as an evaluation metric showed the **best** results across all distributions.

Splitting the data into a **70/30** distribution scored the **lowest** results.

An **80/20** distribution scored the **best** using the Gain Ratio evaluation metric.

A **90/10** distribution scored the **best** using the Information Gain/Gini Index evaluation metrics.

And overall, an **80/20 distribution with a Gain Ratio evaluation metric showed the best results** out of all 6 columns presented above.

This could be attributed to a balanced representation of the data for model training and testing. In a classification task, having a diverse and representative set of examples for both training and testing is crucial for the model to generalize well to unseen data. The 80/20 distribution strikes a balance between having enough data for the model to learn patterns effectively (80% for training) while also having a substantial portion reserved for assessing the model's performance on new, unseen instances (20% for testing). Additionally, the Gain Ratio evaluation metric takes into account the intrinsic characteristics of the dataset, this might be better suited for the nature of our categorical dataset, contributing to the improved performance. It is essential to fine-tune the distribution and evaluation metric based on the dataset's unique properties to achieve optimal classification results.

#### 7.2 Clustering:

From the results, we found that our dataset is unsuitable for clustering since the clusters overlap in all the plots we made, and the results we got in all evaluation metrics were not good, but despite that 2 clusters did show better results as the silhouette and elbow methods suggested in all metrics except BCubed Precision. We believe this is due to the inherent nature of categorical data. An article on Safe Hub Collective stated that "k-Means algorithm is not applicable to categorical data, as categorical variables are discrete and do not have any natural origin. So computing euclidean distance for such as space is not meaningful."[1]

#### 7.3 Classification VS Clustering:

In conclusion, classification appears to be a more suitable approach than clustering for our dataset and problem. The nature of our dataset which contains only categorical attributes, aligns well with the goals of a classification task. Through classification, we aimed to analyze and understand the attributes influencing the rate of adaptivity level. The results demonstrated consistent and promising performance across various evaluation metrics, indicating that the classification models were effective in predicting and categorizing adaptivity levels.

On the other hand, clustering, as evidenced by the unsuitability of our dataset for this task, struggled to unveil meaningful patterns due to the inherent characteristics of categorical data. The overlapping clusters in our plots and the limitations of the k-means algorithm for categorical data further support the preference for classification in our context.

The choice of an 80/20 distribution with the Gain Ratio evaluation metric emerged as the optimal strategy for classification. This balanced representation in the training set, along with the consideration of intrinsic dataset characteristics through Gain Ratio, contributed to the models' ability to generalize and perform well on unseen data.

In summary, for our goal of analyzing and enhancing adaptability, classification, particularly with an 80/20 distribution and Gain Ratio evaluation, proves to be a more effective and insightful approach compared to clustering.

### 8. References:

[1] Crispy, "Can you do cluster analysis with categorical variables? -- Safehubcollective.org," Apr. 01, 2020. <https://safehubcollective.org/helpful-tips/can-you-do-cluster-analysis-with-categorical-variables/>
